{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW3 by Linsen Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. N-gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Preprocess the train and validation data, build the vocabulary, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./a3-data/train.txt', \"r\") as f:\n",
    "        train_text = [line for line in f]\n",
    "\n",
    "with open('./a3-data/valid.txt', \"r\") as f:\n",
    "        test_text = [line for line in f]\n",
    "        \n",
    "import nltk\n",
    "train_sentence = ' '.join(train_text).replace('<unk>', '')\n",
    "test_sentence = ' '.join(test_text).replace('<unk>', '')\n",
    "train_tokens = nltk.word_tokenize(train_sentence)\n",
    "test_tokens = nltk.word_tokenize(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built the vocabulary\n",
    "from nltk.lm import Vocabulary\n",
    "vocab1 = Vocabulary(train_tokens, unk_cutoff=2)\n",
    "vocab2 = Vocabulary(test_tokens, unk_cutoff=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9962"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3984"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text.\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "\n",
    "sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "train_tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(train_sentence)]\n",
    "test_tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(test_sentence)]\n",
    "# Preprocess the tokenized text for 3-grams language modelling\n",
    "n = 3\n",
    "train, v_train = padded_everygram_pipeline(3, train_tokenized_text)\n",
    "test, v_test = padded_everygram_pipeline(3, test_tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Implement an N-gram model (trigram) for language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bulid the trigram model\n",
    "from nltk.lm import MLE\n",
    "ngram = MLE(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before training, the vocab of model is 0\n",
    "len(ngram.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the trigram model\n",
    "ngram.fit(train, v_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After training the n-gram model, the vocab of model\n",
    "len(ngram.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', 'pierre', 'n', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov', '.')\n"
     ]
    }
   ],
   "source": [
    "print(ngram.vocab.lookup(train_tokenized_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<NgramCounter with 3 ngram orders and 2544687 ngrams>\n"
     ]
    }
   ],
   "source": [
    "# Show the n-gram model\n",
    "print(ngram.counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08285479901558655"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check some probability\n",
    "ngram.score('is', 'this'.split())  # P('is'|'this')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7857142857142857"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check some probability\n",
    "ngram.score('of', 'the type'.split())  # P('of'|'the type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Implement Good Turing smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Good Turing estimation\n",
    "from nltk import SimpleGoodTuringProbDist,FreqDist \n",
    "import nltk\n",
    "ngrams = nltk.trigrams(train_sentence)\n",
    "freq_dist = nltk.FreqDist(ngrams)\n",
    "Good_turing = SimpleGoodTuringProbDist(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SimpleGoodTuringProbDist based on 4918583 samples>\n"
     ]
    }
   ],
   "source": [
    "from nltk.model.ngram import NgramModel\n",
    "est = lambda freq_dist: SimpleGoodTuringProbDist(freq_dist)\n",
    "gt = NgramModel(3, corpus, estimator=est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) Implement Kneser-Ney Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use kneser_ney estimation\n",
    "import nltk\n",
    "ngrams = nltk.trigrams(train_sentence)\n",
    "freq_dist = nltk.FreqDist(ngrams)\n",
    "kneser_ney = nltk.KneserNeyProbDist(freq_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KneserNeyProbDist based on 4918583 trigrams\n"
     ]
    }
   ],
   "source": [
    "print(kneser_ney)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use nltk package to implement Kneser-Ney Smoothing\n",
    "from nltk.lm.models import KneserNeyInterpolated,Lidstone\n",
    "from nltk.lm.smoothing import KneserNey, WittenBell\n",
    "from nltk.lm.api import Smoothing\n",
    "kn = KneserNeyInterpolated(3)\n",
    "kn.fit(train, v_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After training the n-gram model, the vocab of model\n",
    "len(kn.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Predict the next word in the valid set using a sliding window. Report the perplexity scores of N-gram, Good Turing, and Kneser-Ney on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sliding window on valid set\n",
    "# Set the window size as 20\n",
    "# So we are using previous 19 words to predict the 20th word\n",
    "import numpy as np\n",
    "windows = list()\n",
    "for i in range(19, len(test_tokens)):\n",
    "    window = test_tokens[i-19: i+1]\n",
    "    windows.append(window)\n",
    "previous_19 = [i[:-1] for i in windows]\n",
    "actual_20 = [''.join(i[-1:]) for i in windows]\n",
    "# Predict the 20th word basing on the previous 19 word\n",
    "# All prediction are stored in the list predict_word\n",
    "predict_word = [ngram.generate(1,text_seed=i, random_seed=3) for i in previous_19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predict word: \n",
      "['football', 'explains', 'be', 'bring', 'for', 'market', 'independence', 'few', 'contribution', 'between']\n",
      "\n",
      "The actual word in test set:\n",
      "['football', 'can', 'now', 'vote', 'during', 'for', 'the', 'greatest', 'play', 'in']\n"
     ]
    }
   ],
   "source": [
    "# Show some predict result \n",
    "# We only show first 10 predict in this case\n",
    "print('The predict word: ')\n",
    "print(predict_word[:10])\n",
    "print('\\nThe actual word in test set:')\n",
    "print(actual_20[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of Predicting the next word in sliding window : 0.096465\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy\n",
    "num = 0\n",
    "for i in range(len(predict_word)):\n",
    "    if actual_20[i] == predict_word[i]:\n",
    "        num += 1\n",
    "print('The accuracy of Predicting the next word in sliding window : %f' % (num/len(predict_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The perplexity scores of ngram model on test set is :27576.069826\n"
     ]
    }
   ],
   "source": [
    "# To calculate the perplexity scores on the test set\n",
    "p_ngram = np.array([ngram.perplexity(i) for i in test_tokens])\n",
    "per_ngram = np.ma.masked_invalid(p).mean()\n",
    "print('The perplexity scores of ngram model on test set is :%f' % per_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Choose the first 30 lines and print the predictions of next words using N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"but while the new york stock exchange did n't fall ___\\n\",\n",
       " 'some circuit breakers installed after the october N crash failed ___\\n',\n",
       " 'the N stock specialist firms on the big board floor ___\\n',\n",
       " 'big investment banks refused to step up to the plate ___\\n',\n",
       " \"heavy selling of standard & poor 's 500-stock index futures ___\\n\",\n",
       " 'seven big board stocks ual amr bankamerica walt disney capital ___\\n',\n",
       " 'once again the specialists were not able to handle the ___\\n',\n",
       " '<unk> james <unk> chairman of specialists henderson brothers inc. it ___\\n',\n",
       " 'when the dollar is in a <unk> even central banks ___\\n',\n",
       " 'speculators are calling for a degree of liquidity that is ___\\n',\n",
       " 'many money managers and some traders had already left their ___\\n',\n",
       " 'then in a <unk> plunge the dow jones industrials in ___\\n',\n",
       " '<unk> trading accelerated to N million shares a record for ___\\n',\n",
       " 'at the end of the day N million shares were ___\\n',\n",
       " \"the dow 's decline was second in point terms only ___\\n\",\n",
       " \"in percentage terms however the dow 's dive was the ___\\n\",\n",
       " 'shares of ual the parent of united airlines were extremely ___\\n',\n",
       " \"wall street 's takeover-stock speculators or risk arbitragers had placed ___\\n\",\n",
       " 'at N p.m. edt came the <unk> news the big ___\\n',\n",
       " 'on the exchange floor as soon as ual stopped trading ___\\n',\n",
       " 'several traders could be seen shaking their heads when the ___\\n',\n",
       " 'for weeks the market had been nervous about takeovers after ___\\n',\n",
       " 'and N minutes after the ual trading halt came news ___\\n',\n",
       " \"arbitragers could n't dump their ual stock but they rid ___\\n\",\n",
       " 'for example their selling caused trading halts to be declared ___\\n',\n",
       " 'but as panic spread speculators began to sell blue-chip stocks ___\\n',\n",
       " 'when trading was halted in philip morris the stock was ___\\n',\n",
       " 'selling <unk> because of waves of automatic stop-loss orders which ___\\n',\n",
       " 'most of the stock selling pressure came from wall street ___\\n',\n",
       " 'traders said most of their major institutional investors on the ___\\n']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the predictions of next words in the same 30 lines of input.txt as in N-gram.\n",
    "# We first print out the sentence we want to predict\n",
    "with open('./a3-data/input.txt', \"r\") as f:\n",
    "        prediction_text = [line for line in f]\n",
    "top_30 = prediction_text[0:30]\n",
    "top_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "def ngram_predict(text):\n",
    "    # Tokenize the test to a list \n",
    "    # exclude the last word which we are to predict\n",
    "    token = word_tokenize(text)[:-1]\n",
    "    predict_word = ngram.generate(1,text_seed=token, random_seed=3)\n",
    "    result = text.rstrip() + ': ' + predict_word\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "but while the new york stock exchange did n't fall ___: by\n",
      "\n",
      "Example 2:\n",
      "some circuit breakers installed after the october N crash failed ___: labor-management\n",
      "\n",
      "Example 3:\n",
      "the N stock specialist firms on the big board floor ___: at\n",
      "\n",
      "Example 4:\n",
      "big investment banks refused to step up to the plate ___: and\n",
      "\n",
      "Example 5:\n",
      "heavy selling of standard & poor 's 500-stock index futures ___: contract\n",
      "\n",
      "Example 6:\n",
      "seven big board stocks ual amr bankamerica walt disney capital ___: for\n",
      "\n",
      "Example 7:\n",
      "once again the specialists were not able to handle the ___: abortion\n",
      "\n",
      "Example 8:\n",
      "<unk> james <unk> chairman of specialists henderson brothers inc. it ___: 's\n",
      "\n",
      "Example 9:\n",
      "when the dollar is in a <unk> even central banks ___: notably\n",
      "\n",
      "Example 10:\n",
      "speculators are calling for a degree of liquidity that is ___: creating\n",
      "\n",
      "Example 11:\n",
      "many money managers and some traders had already left their ___: demand\n",
      "\n",
      "Example 12:\n",
      "then in a <unk> plunge the dow jones industrials in ___: europe\n",
      "\n",
      "Example 13:\n",
      "<unk> trading accelerated to N million shares a record for ___: a\n",
      "\n",
      "Example 14:\n",
      "at the end of the day N million shares were ___: issued\n",
      "\n",
      "Example 15:\n",
      "the dow 's decline was second in point terms only ___: by\n",
      "\n",
      "Example 16:\n",
      "in percentage terms however the dow 's dive was the ___: first\n",
      "\n",
      "Example 17:\n",
      "shares of ual the parent of united airlines were extremely ___: pleased\n",
      "\n",
      "Example 18:\n",
      "wall street 's takeover-stock speculators or risk arbitragers had placed ___: convertible\n",
      "\n",
      "Example 19:\n",
      "at N p.m. edt came the <unk> news the big ___: board\n",
      "\n",
      "Example 20:\n",
      "on the exchange floor as soon as ual stopped trading ___: altogether\n",
      "\n",
      "Example 21:\n",
      "several traders could be seen shaking their heads when the ___: country\n",
      "\n",
      "Example 22:\n",
      "for weeks the market had been nervous about takeovers after ___: eye\n",
      "\n",
      "Example 23:\n",
      "and N minutes after the ual trading halt came news ___: of\n",
      "\n",
      "Example 24:\n",
      "arbitragers could n't dump their ual stock but they rid ___: of\n",
      "\n",
      "Example 25:\n",
      "for example their selling caused trading halts to be declared ___: a\n",
      "\n",
      "Example 26:\n",
      "but as panic spread speculators began to sell blue-chip stocks ___: including\n",
      "\n",
      "Example 27:\n",
      "when trading was halted in philip morris the stock was ___: clothing\n",
      "\n",
      "Example 28:\n",
      "selling <unk> because of waves of automatic stop-loss orders which ___: are\n",
      "\n",
      "Example 29:\n",
      "most of the stock selling pressure came from wall street ___: brokerage\n",
      "\n",
      "Example 30:\n",
      "traders said most of their major institutional investors on the ___: corsica\n"
     ]
    }
   ],
   "source": [
    "# Print out the prediction of next word using n-gram model\n",
    "for i in range(30):\n",
    "    print('\\nExample %d:' % (i+1))\n",
    "    ngram_predict(top_30[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a),(b),(c),(d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9649\n",
      "Total Sequences: 842605\n",
      "Window size: 20\n"
     ]
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "# In order to use RNN model to predict the next word\n",
    "# Use integer encode text\n",
    "# First generate training set\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([train_sentence])\n",
    "encoded = tokenizer.texts_to_sequences([train_sentence])[0]\n",
    "# Determine the vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "\n",
    "# Set the window size as 20\n",
    "sequences = list()\n",
    "for i in range(19, len(encoded)):\n",
    "    sequence = encoded[i-19:i+1]\n",
    "    sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "# pad sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Window size: %d' % max_length)\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X_train, y = sequences[:,:-1],sequences[:,-1]\n",
    "y_train = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 9649\n",
      "Total Sequences: 66834\n",
      "Window size: 20\n"
     ]
    }
   ],
   "source": [
    "# Generate the test set\n",
    "# Use the same approach above\n",
    "\n",
    "tokenizer.fit_on_texts([test_sentence])\n",
    "encoded_2 = tokenizer.texts_to_sequences([test_sentence])[0]\n",
    "# determine the vocabulary size\n",
    "vocab_size_2 = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size_2)\n",
    "\n",
    "# Set the window of 20\n",
    "sequences = list()\n",
    "for i in range(19, len(encoded_2)):\n",
    "    sequence = encoded_2[i-19:i+1]\n",
    "    sequences.append(sequence)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "# pad sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Window size: %d' % max_length)\n",
    "# split into input and output elements\n",
    "sequences = array(sequences)\n",
    "X_test, y = sequences[:,:-1],sequences[:,-1]\n",
    "y_test = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X_train: (842605, 19)\n",
      "shape of y_train: (842605, 9649)\n",
      "shape of X_test: (66834, 19)\n",
      "shape of y_test: (66834, 9649)\n"
     ]
    }
   ],
   "source": [
    "# Print out the shape of each part \n",
    "print('shape of X_train: ' + str(X_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of X_test: ' + str(X_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 19, 10)            96490     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 60)                17040     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9649)              588589    \n",
      "=================================================================\n",
      "Total params: 702,119\n",
      "Trainable params: 702,119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Set up the structure of RNN model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_length-1))\n",
    "model.add(LSTM(60))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcualte perplexity\n",
    "import keras.backend as kb\n",
    "def perplexity(y_predict, y_label):\n",
    "    ce = kb.categorical_crossentropy(y_predict, y_label)\n",
    "    return kb.exp(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "842605/842605 [==============================] - 483s 573us/step - loss: 5.6951 - perplexity: 93405.6094\n",
      "Epoch 2/50\n",
      "842605/842605 [==============================] - 443s 526us/step - loss: 5.6126 - perplexity: 151171.9688\n",
      "Epoch 3/50\n",
      "842605/842605 [==============================] - 423s 502us/step - loss: 5.5860 - perplexity: 51704.7227\n",
      "Epoch 4/50\n",
      "842605/842605 [==============================] - 418s 496us/step - loss: 5.5723 - perplexity: 116320.7734\n",
      "Epoch 5/50\n",
      "842605/842605 [==============================] - 424s 503us/step - loss: 5.5665 - perplexity: 44029.1289\n",
      "Epoch 6/50\n",
      "842605/842605 [==============================] - 409s 485us/step - loss: 5.5612 - perplexity: 348676.7500\n",
      "Epoch 7/50\n",
      "842605/842605 [==============================] - 409s 486us/step - loss: 5.5558 - perplexity: 183300.5469\n",
      "Epoch 8/50\n",
      "842605/842605 [==============================] - 409s 485us/step - loss: 5.5518 - perplexity: 785979.1875\n",
      "Epoch 9/50\n",
      "842605/842605 [==============================] - 409s 485us/step - loss: 5.5497 - perplexity: 107168.9453\n",
      "Epoch 10/50\n",
      "842605/842605 [==============================] - 410s 486us/step - loss: 5.5474 - perplexity: 3712405.0000\n",
      "Epoch 11/50\n",
      "842605/842605 [==============================] - 409s 485us/step - loss: 5.5442 - perplexity: 544984.8125TA: 1s - loss: 5.5443 - perp\n",
      "Epoch 12/50\n",
      "842605/842605 [==============================] - 408s 484us/step - loss: 5.5431 - perplexity: 36231.5039\n",
      "Epoch 13/50\n",
      "842605/842605 [==============================] - 407s 483us/step - loss: 5.5417 - perplexity: 73180400.0000\n",
      "Epoch 14/50\n",
      "842605/842605 [==============================] - 412s 489us/step - loss: 5.5391 - perplexity: 6515043.5000\n",
      "Epoch 15/50\n",
      "842605/842605 [==============================] - 409s 485us/step - loss: 5.5387 - perplexity: 372609.0625\n",
      "Epoch 16/50\n",
      "842605/842605 [==============================] - 411s 488us/step - loss: 5.5372 - perplexity: 46501.9180\n",
      "Epoch 17/50\n",
      "842605/842605 [==============================] - 406s 482us/step - loss: 5.5358 - perplexity: 1479121.1250 - loss: 5.5357 - perplexity\n",
      "Epoch 18/50\n",
      "842605/842605 [==============================] - 428s 507us/step - loss: 5.5340 - perplexity: 2150166.0000\n",
      "Epoch 19/50\n",
      "842605/842605 [==============================] - 410s 487us/step - loss: 5.5347 - perplexity: 13724238.0000s - loss: \n",
      "Epoch 20/50\n",
      "842605/842605 [==============================] - 409s 485us/step - loss: 5.5325 - perplexity: 212997.4062\n",
      "Epoch 21/50\n",
      "842605/842605 [==============================] - 415s 492us/step - loss: 5.5318 - perplexity: 604572.0625\n",
      "Epoch 22/50\n",
      "842605/842605 [==============================] - 409s 485us/step - loss: 5.5308 - perplexity: 79473.4453\n",
      "Epoch 23/50\n",
      "842605/842605 [==============================] - 409s 486us/step - loss: 5.5289 - perplexity: 529725120.0000\n",
      "Epoch 24/50\n",
      "842605/842605 [==============================] - 413s 491us/step - loss: 5.5286 - perplexity: 9217492.0000\n",
      "Epoch 25/50\n",
      "842605/842605 [==============================] - 413s 490us/step - loss: 5.5274 - perplexity: 1759286.7500\n",
      "Epoch 26/50\n",
      "842605/842605 [==============================] - 411s 488us/step - loss: 5.5256 - perplexity: 296746.1250\n",
      "Epoch 27/50\n",
      "842605/842605 [==============================] - 411s 488us/step - loss: 5.5269 - perplexity: 259866.3750\n",
      "Epoch 28/50\n",
      "842605/842605 [==============================] - 591s 701us/step - loss: 5.5267 - perplexity: 878493120.0000\n",
      "Epoch 29/50\n",
      "842605/842605 [==============================] - 2478s 3ms/step - loss: 5.5259 - perplexity: 4474704.5000\n",
      "Epoch 30/50\n",
      "842605/842605 [==============================] - 2914s 3ms/step - loss: 5.5246 - perplexity: 1083913.2500\n",
      "Epoch 31/50\n",
      "842605/842605 [==============================] - 5057s 6ms/step - loss: 5.5226 - perplexity: 1787135.6250\n",
      "Epoch 32/50\n",
      "842605/842605 [==============================] - 450s 534us/step - loss: 5.5228 - perplexity: 79130688.0000\n",
      "Epoch 33/50\n",
      "842605/842605 [==============================] - 432s 513us/step - loss: 5.5229 - perplexity: 1833918720.0000\n",
      "Epoch 34/50\n",
      "842605/842605 [==============================] - 425s 504us/step - loss: 5.5229 - perplexity: 432545.8125\n",
      "Epoch 35/50\n",
      "842605/842605 [==============================] - 417s 494us/step - loss: 5.5209 - perplexity: 234452.2344\n",
      "Epoch 36/50\n",
      "842605/842605 [==============================] - 419s 498us/step - loss: 5.5229 - perplexity: 149995.1094\n",
      "Epoch 37/50\n",
      "842605/842605 [==============================] - 422s 501us/step - loss: 5.5223 - perplexity: 24763358.0000\n",
      "Epoch 38/50\n",
      "842605/842605 [==============================] - 435s 516us/step - loss: 5.5214 - perplexity: 41926.2500\n",
      "Epoch 39/50\n",
      "842605/842605 [==============================] - 438s 520us/step - loss: 5.5198 - perplexity: 9863213.0000\n",
      "Epoch 40/50\n",
      "842605/842605 [==============================] - 438s 520us/step - loss: 5.5198 - perplexity: 116609.4297\n",
      "Epoch 41/50\n",
      "842605/842605 [==============================] - 418s 496us/step - loss: 5.5186 - perplexity: 400815.9062\n",
      "Epoch 42/50\n",
      "842605/842605 [==============================] - 413s 491us/step - loss: 5.5191 - perplexity: 1660263363248128.0000\n",
      "Epoch 43/50\n",
      "842605/842605 [==============================] - 420s 498us/step - loss: 5.5180 - perplexity: 4652732.0000\n",
      "Epoch 44/50\n",
      "842605/842605 [==============================] - 442s 524us/step - loss: 5.5171 - perplexity: 1475546.3750\n",
      "Epoch 45/50\n",
      "842605/842605 [==============================] - 445s 528us/step - loss: 5.5165 - perplexity: 2523586.2500\n",
      "Epoch 46/50\n",
      "842605/842605 [==============================] - 435s 517us/step - loss: 5.5167 - perplexity: 1863261.8750\n",
      "Epoch 47/50\n",
      "842605/842605 [==============================] - 418s 496us/step - loss: 5.5163 - perplexity: 38239032.0000\n",
      "Epoch 48/50\n",
      "842605/842605 [==============================] - 497s 589us/step - loss: 5.5163 - perplexity: 577698.8750\n",
      "Epoch 49/50\n",
      "842605/842605 [==============================] - 551s 654us/step - loss: 5.5145 - perplexity: 18777303040.0000\n",
      "Epoch 50/50\n",
      "842605/842605 [==============================] - 520s 617us/step - loss: 5.5154 - perplexity: 10600326.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13010a780>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile network\n",
    "# Set up the training step: use a learning rate of 1e − 3 and an Adam optimizer\n",
    "# Use perplexity as the metrics\n",
    "# Also calculate the cross-entropy using categorical_crossentropy\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=[perplexity])\n",
    "# Fit network\n",
    "model.fit(X_train, y_train, epochs=50, verbose=1,batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This commented part is to load the existing RNN model\n",
    "'''\n",
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) Train your RNN model. Calcuate the model’s perplexity on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66834/66834 [==============================] - 13s 195us/step\n",
      "loss = 5.891780329160139\n",
      "perplexity = 12316.076171875\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "# Calcuate the model’s perplexity on the test set\n",
    "loss_and_acc = model.evaluate(X_test, y_test)\n",
    "loss = loss_and_acc[0]\n",
    "perplexity_test = loss_and_acc[1]\n",
    "print('loss = ' + str(loss))\n",
    "print('perplexity = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12199.006990557798\n"
     ]
    }
   ],
   "source": [
    "# Prove the perplexity equation\n",
    "import numpy as np\n",
    "sample_size = 66834\n",
    "batch_size = 50\n",
    "number_of_predictions = 837\n",
    "\n",
    "total_loss = loss*sample_size/batch_size\n",
    "aim = np.exp(total_loss/number_of_predictions)\n",
    "print(aim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, we find that \n",
    "- perplexity = exp(total_loss/number_of_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) Print the predictions of next words in the same 30 lines of input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./a3-data/input.txt', \"r\") as f:\n",
    "        prediction_text = [line for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"but while the new york stock exchange did n't fall ___\\n\",\n",
       " 'some circuit breakers installed after the october N crash failed ___\\n',\n",
       " 'the N stock specialist firms on the big board floor ___\\n',\n",
       " 'big investment banks refused to step up to the plate ___\\n',\n",
       " \"heavy selling of standard & poor 's 500-stock index futures ___\\n\",\n",
       " 'seven big board stocks ual amr bankamerica walt disney capital ___\\n',\n",
       " 'once again the specialists were not able to handle the ___\\n',\n",
       " '<unk> james <unk> chairman of specialists henderson brothers inc. it ___\\n',\n",
       " 'when the dollar is in a <unk> even central banks ___\\n',\n",
       " 'speculators are calling for a degree of liquidity that is ___\\n',\n",
       " 'many money managers and some traders had already left their ___\\n',\n",
       " 'then in a <unk> plunge the dow jones industrials in ___\\n',\n",
       " '<unk> trading accelerated to N million shares a record for ___\\n',\n",
       " 'at the end of the day N million shares were ___\\n',\n",
       " \"the dow 's decline was second in point terms only ___\\n\",\n",
       " \"in percentage terms however the dow 's dive was the ___\\n\",\n",
       " 'shares of ual the parent of united airlines were extremely ___\\n',\n",
       " \"wall street 's takeover-stock speculators or risk arbitragers had placed ___\\n\",\n",
       " 'at N p.m. edt came the <unk> news the big ___\\n',\n",
       " 'on the exchange floor as soon as ual stopped trading ___\\n',\n",
       " 'several traders could be seen shaking their heads when the ___\\n',\n",
       " 'for weeks the market had been nervous about takeovers after ___\\n',\n",
       " 'and N minutes after the ual trading halt came news ___\\n',\n",
       " \"arbitragers could n't dump their ual stock but they rid ___\\n\",\n",
       " 'for example their selling caused trading halts to be declared ___\\n',\n",
       " 'but as panic spread speculators began to sell blue-chip stocks ___\\n',\n",
       " 'when trading was halted in philip morris the stock was ___\\n',\n",
       " 'selling <unk> because of waves of automatic stop-loss orders which ___\\n',\n",
       " 'most of the stock selling pressure came from wall street ___\\n',\n",
       " 'traders said most of their major institutional investors on the ___\\n']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the predictions of next words in the same 30 lines of input.txt as in N-gram.\n",
    "# We first print out the sentence we want to predict\n",
    "top_30 = prediction_text[0:30]\n",
    "top_30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a sequence of words, predict the next word using RNN\n",
    "def rnn_predict(text):\n",
    "    encoded = tokenizer.texts_to_sequences([text])[0]\n",
    "    # pre-pad sequences to a fixed length\n",
    "    encoded = pad_sequences([encoded], maxlen=max_length-1, padding='pre')\n",
    "    # predict probabilities for each word\n",
    "    y_predict_class = model.predict_classes(encoded, verbose=0)\n",
    "    # map predicted word index to word\n",
    "    predict_word = ''\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == y_predict_class:\n",
    "            predict_word = word\n",
    "            break\n",
    "    # append to input\n",
    "    result = text.rstrip() + ': ' + predict_word\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "but while the new york stock exchange did n't fall ___: the\n",
      "\n",
      "Example 2:\n",
      "some circuit breakers installed after the october N crash failed ___: the\n",
      "\n",
      "Example 3:\n",
      "the N stock specialist firms on the big board floor ___: 's\n",
      "\n",
      "Example 4:\n",
      "big investment banks refused to step up to the plate ___: of\n",
      "\n",
      "Example 5:\n",
      "heavy selling of standard & poor 's 500-stock index futures ___: the\n",
      "\n",
      "Example 6:\n",
      "seven big board stocks ual amr bankamerica walt disney capital ___: to\n",
      "\n",
      "Example 7:\n",
      "once again the specialists were not able to handle the ___: company\n",
      "\n",
      "Example 8:\n",
      "<unk> james <unk> chairman of specialists henderson brothers inc. it ___: 's\n",
      "\n",
      "Example 9:\n",
      "when the dollar is in a <unk> even central banks ___: of\n",
      "\n",
      "Example 10:\n",
      "speculators are calling for a degree of liquidity that is ___: the\n",
      "\n",
      "Example 11:\n",
      "many money managers and some traders had already left their ___: the\n",
      "\n",
      "Example 12:\n",
      "then in a <unk> plunge the dow jones industrials in ___: the\n",
      "\n",
      "Example 13:\n",
      "<unk> trading accelerated to N million shares a record for ___: the\n",
      "\n",
      "Example 14:\n",
      "at the end of the day N million shares were ___: than\n",
      "\n",
      "Example 15:\n",
      "the dow 's decline was second in point terms only ___: n\n",
      "\n",
      "Example 16:\n",
      "in percentage terms however the dow 's dive was the ___: company\n",
      "\n",
      "Example 17:\n",
      "shares of ual the parent of united airlines were extremely ___: of\n",
      "\n",
      "Example 18:\n",
      "wall street 's takeover-stock speculators or risk arbitragers had placed ___: the\n",
      "\n",
      "Example 19:\n",
      "at N p.m. edt came the <unk> news the big ___: of\n",
      "\n",
      "Example 20:\n",
      "on the exchange floor as soon as ual stopped trading ___: the\n",
      "\n",
      "Example 21:\n",
      "several traders could be seen shaking their heads when the ___: this\n",
      "\n",
      "Example 22:\n",
      "for weeks the market had been nervous about takeovers after ___: the\n",
      "\n",
      "Example 23:\n",
      "and N minutes after the ual trading halt came news ___: the\n",
      "\n",
      "Example 24:\n",
      "arbitragers could n't dump their ual stock but they rid ___: the\n",
      "\n",
      "Example 25:\n",
      "for example their selling caused trading halts to be declared ___: the\n",
      "\n",
      "Example 26:\n",
      "but as panic spread speculators began to sell blue-chip stocks ___: and\n",
      "\n",
      "Example 27:\n",
      "when trading was halted in philip morris the stock was ___: the\n",
      "\n",
      "Example 28:\n",
      "selling <unk> because of waves of automatic stop-loss orders which ___: the\n",
      "\n",
      "Example 29:\n",
      "most of the stock selling pressure came from wall street ___: to\n",
      "\n",
      "Example 30:\n",
      "traders said most of their major institutional investors on the ___: company\n"
     ]
    }
   ],
   "source": [
    "# Print the predictions of next words in the same 30 lines of input.txt as in N-gram.\n",
    "for i in range(30):\n",
    "    text = top_30[i]\n",
    "    print('\\nExample ' + str(i+1) + ':')\n",
    "    rnn_predict(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
